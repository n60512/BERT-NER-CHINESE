{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599207900381",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets.make_cner import make_cner\n",
    "\n",
    "from datasets.datasets import CNERDataset\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "# Get model tokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "words = list()\n",
    "entities = list()\n",
    "res = list()\n",
    "\n",
    "sen_index = 0\n",
    "\n",
    "\n",
    "with open('data/cner/dev.char.bmes', 'r', encoding='utf-8') as f:\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        l = line.split(' ')\n",
    "\n",
    "        if len(l)>1:\n",
    "            w = l[0]\n",
    "            e = l[1].replace('\\n', '')\n",
    "\n",
    "            words.append(w)\n",
    "            entities.append(e)\n",
    "\n",
    "            res.append((sen_index, w, e))\n",
    "        else:\n",
    "            sen_index += 1\n",
    "            pass\n",
    "\n",
    "    f.close()\n",
    "    pass\n",
    "\n",
    "res[:10]\n",
    "# words[:180], entities[:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities = [val[2] for val in res]\n",
    "entities_label = list(set(entities))\n",
    "\n",
    "\n",
    "index2entities = {i:val for i, val in enumerate(entities_label)}\n",
    "index2entities[len(index2entities)] = '[PAD]'\n",
    "index2entities[len(index2entities)] = '[CLS]'\n",
    "index2entities[len(index2entities)] = '[SEP]'\n",
    "entities2index = {v:i for i, v in index2entities.items()}\n",
    "\n",
    "index2entities, entities2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_res = [(i, w, entities2index[e]) for i, w, e in res]\n",
    "_res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "tokens_sen = list()\n",
    "tokens_ent = list()\n",
    "for i, w, e in _res:\n",
    "    if(i==idx):\n",
    "        tokens_sen.append(w)\n",
    "        tokens_ent.append(e)\n",
    "    pass\n",
    "\n",
    "# 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "word_pieces = [\"[CLS]\"]\n",
    "# tokens_sen = self.tokenizer.tokenize(sen)\n",
    "word_pieces += tokens_sen + [\"[SEP]\"]\n",
    "\n",
    "entity_pieces = [entities2index[\"[CLS]\"]]\n",
    "entity_pieces += tokens_ent + [entities2index[\"[SEP]\"]]\n",
    "\n",
    "word_pieces, entity_pieces, len(word_pieces), len(entity_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cner_dataset = CNERDataset('dev', tokenizer, 'data/cner/test.char.bmes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cner_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2entities[25],index2entities[26], cner_dataset.index2entities[25], cner_dataset.index2entities[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import create_mini_batch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cner_dataset, \n",
    "    batch_size=4, \n",
    "    collate_fn=create_mini_batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index2entities, entities2index = make_cner('data/cner/dev.char.bmes', _mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "({1: 'B-LOC',\n  2: 'M-CONT',\n  3: 'B-ORG',\n  4: 'E-ORG',\n  5: 'E-LOC',\n  6: 'B-EDU',\n  7: 'B-PRO',\n  8: 'E-TITLE',\n  9: 'E-RACE',\n  10: 'M-TITLE',\n  11: 'O',\n  12: 'M-ORG',\n  13: 'S-NAME',\n  14: 'B-RACE',\n  15: 'E-CONT',\n  16: 'B-TITLE',\n  17: 'S-RACE',\n  18: 'M-PRO',\n  19: 'M-EDU',\n  20: 'M-LOC',\n  21: 'B-NAME',\n  22: 'M-NAME',\n  23: 'E-NAME',\n  24: 'E-PRO',\n  25: 'B-CONT',\n  26: 'E-EDU',\n  0: '[PAD]',\n  27: '[CLS]',\n  28: '[SEP]'},\n {'B-LOC': 1,\n  'M-CONT': 2,\n  'B-ORG': 3,\n  'E-ORG': 4,\n  'E-LOC': 5,\n  'B-EDU': 6,\n  'B-PRO': 7,\n  'E-TITLE': 8,\n  'E-RACE': 9,\n  'M-TITLE': 10,\n  'O': 11,\n  'M-ORG': 12,\n  'S-NAME': 13,\n  'B-RACE': 14,\n  'E-CONT': 15,\n  'B-TITLE': 16,\n  'S-RACE': 17,\n  'M-PRO': 18,\n  'M-EDU': 19,\n  'M-LOC': 20,\n  'B-NAME': 21,\n  'M-NAME': 22,\n  'E-NAME': 23,\n  'E-PRO': 24,\n  'B-CONT': 25,\n  'E-EDU': 26,\n  '[PAD]': 0,\n  '[CLS]': 27,\n  '[SEP]': 28})"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "index2entities, entities2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Get model tokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained(PRETRAINED_MODEL_NAME, return_dict=True)\n",
    "inputs = tokenizer(\"但既然大神都這樣建議了，那肯定不會有錯的。\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "({'input_ids': tensor([[ 101,  852, 3188, 4197, 1920, 4868, 6963, 6857, 3564, 2456, 6359,  749,\n          8024, 6929, 5507, 2137,  679, 3298, 3300, 7097, 4638,  511,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.4396, grad_fn=<NllLossBackward>),\n tensor([[[-0.2191, -0.0391],\n          [ 0.1303, -0.6352],\n          [-0.2937,  0.2079],\n          [-0.2876, -0.1088],\n          [-0.5426, -0.0326],\n          [-0.8943,  0.2427],\n          [-0.3415,  0.0529],\n          [-0.8706, -0.1366],\n          [-0.3502,  0.1141],\n          [-0.5358, -0.0063],\n          [-0.5272,  0.7046],\n          [-0.1649,  0.4001],\n          [-0.5383,  0.3421],\n          [-0.3403,  0.0428],\n          [-0.7319,  0.6270],\n          [-0.7603,  0.5703],\n          [-0.7431,  0.4733],\n          [-0.8255,  0.9471],\n          [-0.0938,  0.3644],\n          [-0.8089, -0.5504],\n          [-0.7775, -0.0595],\n          [-0.7209,  0.3260],\n          [-0.1873,  0.3622]]], grad_fn=<AddBackward0>),\n {'input_ids': tensor([[ 101,  852, 3188, 4197, 1920, 4868, 6963, 6857, 3564, 2456, 6359,  749,\n          8024, 6929, 5507, 2137,  679, 3298, 3300, 7097, 4638,  511,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "loss, logits, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}